# 房天下爬虫逻辑说明

本项目核心数据来源于“房天下（Fang.com）”的二手房频道。爬虫模块负责实时抓取挂牌房源，并将其清洗、入库，为房价估算模型提供基础数据。

---

## 1. 核心模块结构
- `apps/spider/fang.py`: **FangSpider** 类，负责单个城市/区域的抓取逻辑。
- `apps/spider/runner.py`: **run_all** 函数，负责多任务调度、去重与结果汇总。
- `apps/spider/models.py`: **House** 模型，定义了房源数据的存储结构。

---

## 2. 爬虫关键逻辑

### 2.1 城市与子域名解析
房天下不同城市的 URL 采用子域名形式（如 `wuhan.esf.fang.com`）。
- **解析机制**: 支持中文名称（"武汉"）、全拼（"wuhan"）或缩写（"wh"）。
- **实现方式**: 内部通过映射表结合 `pypinyin` 库，自动将用户输入的区域转换为对应的子域名。

### 2.2 搜索与翻页策略
- **URL 构造**: 基于 `https://{subdomain}.esf.fang.com/house/i3{page}-kw{region}/` 进行抓取。
- **翻页控制**:
  - 默认抓取 3 页（约 180 条新数据）。
  - 若连续 3 页无新数据或发生网络错误，爬虫会自动中止，确保效率与安全。

### 2.3 数据清洗与入库
- **去重机制**: 每一条房源数据在入库前，都会根据其唯一的 URL 在数据库中进行校验。
- **更新策略**:
  - **新房源**: 直接创建新记录。
  - **旧房源**: 若 URL 已存在，则更新其总价、单价等可能变动的字段，保留历史记录。

---

## 3. 性能与限制
- **并发控制**: 使用线程池执行爬虫任务，支持多区域并行抓取。
- **每日上限**: 为了避免对目标网站造成过大压力及规避反爬风险，系统设置了 `max_daily` 每日抓取上限（默认 2000 条）。

---

## 4. 如何扩展
如果你想添加新的数据源（如链家、贝壳）：
1. 在 `apps/spider/` 下新建爬虫脚本。
2. 实现类似 `FangSpider` 的解析接口。
3. 在 `runner.py` 中注册新的爬虫实例即可。
